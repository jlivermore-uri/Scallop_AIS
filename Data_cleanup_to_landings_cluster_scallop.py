#!/usr/bin/env python
# coding: utf-8

# In[1]:


# Connecting modeled data to landings and calculating point values for AIS dataset only

import numpy as np
import pandas as pd
import os, random
import datetime, time
import geopandas as gpd

# Change settings to show more rows & columns
pd.options.display.max_rows=100000
pd.options.display.max_columns=100


# In[2]:


# Set up workspace
seen_path = '/work/pi_pengfei_liu_uri_edu/Scallop/Trained'
unseen_path = '/work/pi_pengfei_liu_uri_edu/Scallop/Unseen'
obs_path = '/work/pi_pengfei_liu_uri_edu/Scallop/Observer'
land_path = '/work/pi_pengfei_liu_uri_edu/Scallop/Landings'
out_path = '/work/pi_pengfei_liu_uri_edu/Scallop'


# In[3]:


# Read in inferred model data
scal_inf = pd.read_csv(os.path.join(unseen_path, 'scallop_unseen_inference.csv'))


# In[4]:


scal_inf.describe()


# In[5]:


# Read in seen trips' data (training data)
scal_trained = pd.read_csv(os.path.join(seen_path, 'all_scallop_trained.csv'))

# Need to pull trip start and end times from NEFOP files
obs_trips = pd.read_csv(os.path.join(obs_path, 'riobtrp.csv'))
obs_trips['Trip_ID'] = obs_trips['TRIPID'] + '-' + obs_trips['YEAR'].astype(str)
obs_trips['DATESAIL']=pd.to_datetime(obs_trips['DATESAIL'], infer_datetime_format=True)
obs_trips['DATELAND']=pd.to_datetime(obs_trips['DATELAND'], infer_datetime_format=True)
scal_trained = scal_trained.merge(obs_trips, left_on='Trip_ID', right_on='Trip_ID')


# In[6]:


scal_trained['VTRSERNO'] = scal_trained['VTRSERNO'].astype(str)
scal_trained['VTRSERNO'].str.len().plot(kind='kde').ticklabel_format(style='plain')


# In[7]:


# Combine all landings files into a single dataframe
# all_land = []

# for newFile in os.listdir(land_path):
#     if newFile.startswith('Landings_'):
#         print(newFile)       
#         thisLand = pd.read_csv(os.path.join(land_path, newFile),encoding = "ISO-8859-1")
#         all_land.append(thisLand)

# # Save the gear files
# all_land = pd.concat(all_land)
# all_land.to_csv(os.path.join(land_path,'all_land_2015-2018.csv'))
all_land = pd.read_csv(os.path.join(land_path, 'all_land_2015-2018.csv'))
all_land = all_land.drop_duplicates(ignore_index=True)


# In[9]:


# Compare VMS speed cutoff to prediction
# Add column for fishing prediction based on speed alone
scal_trained['VMS_Fishing'] = np.where(scal_trained['SOG'] <= 5, 1, 0) #5 knots for scallop (4 for all other gears)
# Compare predictions from VMS speed filter vs. RF AIS prediction 
scal_trained['VMS_Fishing_Compare'] = np.where(scal_trained['VMS_Fishing'] == scal_trained['NEFOP_Fishing'], 1, 0)
print('Percent of time VMS speed cutoff prediction was correct: ' + str(scal_trained['VMS_Fishing_Compare'].mean()))


# In[10]:


# Fix VTR numbers (mixed across 2 columns right now)
# Serial_num - holds paper VTR numbers (real #s 8 digits) and autogenerated numbers from SAFIS for e-vtrs (fake #s ? digits)
# Supplier_trip_id - holds e-VTR numbers (real #s 14 or 16 digits) and autogenerated numbers from SAFIS for paper VTRs (fake #s 7 digits)
scal_inf['Serial num'] = scal_inf['Serial num'].astype(str)
scal_inf['Supplier trip id'] = scal_inf['Supplier trip id'].astype(str)
scal_inf['VTR_NUMBER'] = np.where(scal_inf['Serial num'].str.len() == 8, scal_inf['Serial num'], scal_inf['Supplier trip id'])


# In[11]:


# Combine seen and unseen scallop data together
scal_trained['Source'] = 'NEFOP'
scal_trained.rename(columns={'GEAR': 'Gear', 'Trip_ID': 'NEFOP_Trip_ID', 'NEFOP_Fishing': 'Fishing_Status',
                            'PERMIT': 'Permit', 'DATESAIL': 'Date_Sail', 'DATELAND': 'Date_Land', 'VTRSERNO': 'VTR_NUMBER'}, inplace=True)
scal_trained = scal_trained[['Datetime','SOG','Depth_m','LON','LAT','COG','NEFOP_Trip_ID','VTR_NUMBER',
                             'Gear','Declaration_code','Length','Permit','MMSI','CallSign','d_SOG','d_Time',
                             'd_Depth','SOG_Avg','SOG_Std','Depth_Avg','Depth_Std','Km_bw_pts','Crow_flies_km',
                             'Total_km_trav','d_COG','d_COG_StartEnd','COG_Avg_Abs_d','Month','Year','Weekday','Date',
                             'Moon','Fishing_Status','Source','Date_Sail','Date_Land']]

scal_inf['NEFOP_Trip_ID'] = -1
scal_inf['Source'] = 'Unseen'
scal_inf.rename(columns={'Predict_Fishing': 'Fishing_Status', 'VTR_Start_date': 'Date_Sail', 
                         'VTR_Land_date': 'Date_Land'}, inplace=True)
scal_inf = scal_inf[['Datetime','SOG','Depth_m','LON','LAT','COG','NEFOP_Trip_ID','VTR_NUMBER',
                     'Gear','Declaration_code','Length','Permit','MMSI','CallSign','d_SOG','d_Time',
                     'd_Depth','SOG_Avg','SOG_Std','Depth_Avg','Depth_Std','Km_bw_pts','Crow_flies_km',
                     'Total_km_trav','d_COG','d_COG_StartEnd','COG_Avg_Abs_d','Month','Year','Weekday','Date',
                     'Moon','Fishing_Status','Source','Date_Sail','Date_Land']]
allScal = pd.concat([scal_trained, scal_inf])
allScal


# In[12]:


# Merge to landings data by VTR number
allDat = allScal.merge(all_land, left_on='VTR_NUMBER', right_on='Supplier Trip Id')
allDat


# In[13]:


allDat.to_csv(os.path.join(out_path,'all_scallop_out.csv'),index=False)


# In[14]:


# Read in allDat if already run all previous code prior (can take a while based on CPU availability)
#allDat = pd.read_csv(os.path.join(out_path, 'all_scallop_out.csv'))


# In[15]:


# Save only data where fishing is occuring (or is predicted to be occuring)
scalFishing = allDat[allDat['Fishing_Status'] == 1]
scalFishing.head()


# In[16]:


scalFishing = scalFishing.drop_duplicates()
# Describe breakdown of NEFOP data vs. Unseen data coverage
countFishingSeen = scalFishing['Source'].value_counts()['NEFOP']
countFishingUnseen = scalFishing['Source'].value_counts()['Unseen']
countAllDatSeen = allDat['Source'].value_counts()['NEFOP']
countAllDatUnseen = allDat['Source'].value_counts()['Unseen']
    
seenFishingRatio = countFishingSeen/countFishingUnseen
seenAllDatRatio = countAllDatSeen/countAllDatUnseen
print('Percent of fishing points observed: ', seenFishingRatio)
print('Percent of total locations (fishing and non) points observed: ', seenAllDatRatio)


# In[17]:


# Need to distribute fishing value (within species and grade) across points in a fishing trip 

# Make sure all points fall within the study area - map bounds from RFP: -72.45 - -69.59 W and 40.1 - 42.1 N
scalFishing = scalFishing[(scalFishing['LON']>=-72.45) & (scalFishing['LON']<=-69.59)]
scalFishing = scalFishing[(scalFishing['LAT']>=40.1) & (scalFishing['LAT']<=42.1)]

# Drop points that fall within state waters because fishing activity being attributed to ports. Further, VERY little activity attributed to state waters in VTRs
fedWaters = gpd.read_file(os.path.join(out_path,'Federal_Waters_Only.shp'))
fedWaters = fedWaters.to_crs('EPSG:4326')

points = gpd.GeoDataFrame(scalFishing, geometry=gpd.points_from_xy(scalFishing.LON, scalFishing.LAT), crs='EPSG:4326')

within_points = gpd.sjoin(points, fedWaters, predicate = 'intersects')

scalFishing = pd.DataFrame(within_points)
scalFishing.head()


# In[19]:


# Saving this interim step so we can just load it and pick up here later
# scalFishing.to_csv(os.path.join(out_path,'interim_scallop_pt_values_AIS.csv'),index=False)
# scalFishing = pd.read_csv(out_path,'interim_scallop_pt_values_AIS.csv')


# In[20]:


# First create a trip ID that includes species+grade because we'll have duplicate values for trips if they sold more than 1 thing to dealer (highly likely)
# Also need to include port and dealer because they may sell to more than one dealer in a single port, and may also stop in multiple ports
scalFishing['UniqueTripTime'] = scalFishing['VTR_NUMBER'].astype(str) + scalFishing['Datetime'].astype(str)

# Found a few VMS trips with duplicate timestamps so correct this first
scalFishing['DuplicateTimeCheck'] = scalFishing['VTR_NUMBER'].astype(str) + scalFishing['Datetime'].astype(str) + scalFishing['Dollars'].astype(str) + scalFishing['Market desc'].astype(str)
scalFishing = scalFishing.drop_duplicates(subset=['DuplicateTimeCheck'])

scalFishing['Num_Landing'] = scalFishing.UniqueTripTime.map(scalFishing.UniqueTripTime.value_counts())
scalFishing['All_Pts_Num'] = scalFishing.VTR_NUMBER.map(scalFishing.VTR_NUMBER.value_counts())
scalFishing['Num_Pts'] = scalFishing['All_Pts_Num']/scalFishing['Num_Landing']

# Generate point 'value' by dividing ex-vessel value by number of points per trip
scalFishing['Point_Value'] = scalFishing['Dollars'] / scalFishing['Num_Pts'].astype(int)
scalFishing.head()


# In[101]:


# Test a few random trips to make sure they add up correctly - run this a bunch of times to make sure it always checks out
scalFishing['VTR_NUMBER'] = scalFishing['VTR_NUMBER'].astype(str)
randomTrip = scalFishing['VTR_NUMBER'].sample().astype(str)
randomTripVal = randomTrip.tolist()
randomTripVal = randomTripVal[0]
print('Random VTR Number: ' + randomTripVal)

randomTrip_vals = scalFishing[(scalFishing['VTR_NUMBER']==randomTripVal)]
print('Point total from AIS: '+ randomTrip_vals['Point_Value'].sum().astype(str))

scalFishingTest2 = randomTrip_vals[['VTR_NUMBER','Dollars']]
scalFishingTest2 = scalFishingTest2.drop_duplicates()
print('Trip value from VTR: '+ scalFishingTest2['Dollars'].sum().astype(str))

# Checks out!


# In[20]:


scalFishing.to_csv(os.path.join(out_path,'scallop_pt_values_AIS.csv'),index=False)


# In[ ]:


# Next script will generate lists of trips and conduct fallbacks: 1) not in AIS - so VMS+VTR only, and 2) not in AIS or VMS - so VTR only
# Mapping of products will be next script after all data compiled together (done in R because of file formats provided by NEFSC for VTRs). 

